# -*- coding: utf-8 -*-
"""BERT TAGGING MultiLabel BERT w/ Rel/Not Test Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h79HYuIdzoRumaW5FPRqqnEjghf81UAy

https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b

# Install & Import Libraries
"""

! pip install -q transformers
! pip install -q pytorch-lightning
! pip install -q sklearn
! pip install -q beautifulsoup4

! pip install -q bs4

# Commented out IPython magic to ensure Python compatibility.
# Import all libraries
import pandas as pd
import numpy as np
import re

# Huggingface transformers
import transformers
from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup

import torch
from torch import nn ,cuda
from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

#handling html data
from bs4 import BeautifulSoup

import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
# %matplotlib inline

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""# Load Datasets"""

from google.colab import drive
drive.mount('/content/drive')

#this will be the output from rel/not rel classifier
test_data_fp = '/content/drive/MyDrive/Colab Notebooks/cPASS/Multi-Label Classifier(s)/Part2_Rel_Input/Data/Rel_Or_Not_Predictions.csv'
train_data_fp = '/content/drive/MyDrive/Colab Notebooks/cPASS/data/Combined.csv'
tags_fp = '/content/drive/MyDrive/Colab Notebooks/cPASS/Multi-Label Classifier(s)/Transformers BERT/Tagsdf.csv'

df = pd.read_csv(train_data_fp, dtype={'speech_id':'str'})
# drop all rows that are tweets
df = df[~df.tweet.astype('bool')]
# convert to boolean column
df.Evaluate = df.Evaluate == 'Yes'
# keep only rows to evaluate
df = df[df.Evaluate]

# test data -- do this in a bit 
test_df = pd.read_csv(test_data_fp, dtype={'speech_id':'str'})

#keep relevant portion of speech only
query = r'((?:\S+\s){0,25})(\*{4}\s.+?\s\*{4})((?:\s\S+){0,300})' #can change number of words to use: 300
def find_keyword(text):
    result = re.search(
        query, text.translate(str.maketrans('', '', '.,!:;\n'))
    )
    if result:
        return result.group(0).replace('****', '').replace('  ', ' ')
    return re.search(
        r'^\S+(\s\S+){0,300}', text.translate(str.maketrans('', '', '.,!:;\n'))
    ) # returns first 300 words

df['speech'] = df['speech'].astype(str)
df.speech = (df.speech.apply(find_keyword))
df = df.reset_index()

df.tail()

speeches = df["speech"]
speeches

#get relevent columns 
df = df[["speech",'Advocates_for_Use_of_American_Military_Force',
 'Advocates_for_Use_of_American_Ground_Troops',
 'Advocates_for_Use_of_American_Air_Assets',
 'Advocates_for_Use_of_American_Naval_Assets',
 'Advocates_for_Money_Military_aid_or_sanctions',
 'Advocates_against_Use_of_American_Military_Force',
 'Advocates_against_Use_of_American_Ground_Troops',
 'Advocates_against_Use_of_American_Air_Assets',
 'Advocates_against_Use_of_American_Naval_Assets',
 'Advocates_against_Money_Military_aid_or_sanctions',
 'President_already_has_sufficient_authority',
 'President_does_not_have_sufficient_authority',
 'Adovcates_for_granting_more_authority',
 'Adovcates_against_granting_more_authority',
 'Adovcates_for_restricting_authority',
 'Adovcates_against_restricting_authority']]
df.head()

"""If there is a one in a column, replace it with the column name """

for col in df.columns:
  if col != 'speech': 
    df.loc[df[col]==1, col] = col

df = df.set_index('speech')

all_tags = []
for i in range(df.shape[0]):
  tags_per_speech = list(df.iloc[i].values)
  all_tags.append(tags_per_speech)

#remove 0s from tags 
all_tags_cleaned = []
for tag_list in all_tags: 
  cleaned_tag_list = []
  for tag in tag_list: 
    if (tag != 0) & (tag != 0.0):
      cleaned_tag_list.append(tag)
  all_tags_cleaned.append(cleaned_tag_list)
#all_tags_cleaned

df = df.reset_index()
df = df[["speech"]].assign(tags = pd.Series(all_tags_cleaned))
df.tail()

df["speech"] = df["speech"].astype(str)

#change this part later to be more accurate 
#restrict each speech to the first 300 words 
df["speech"] = df["speech"].apply(lambda speech: speech[:300])
df.tail()

def to_drop(ser):
  if len(ser)==0:
    return True
  return False

df = df.assign(drop_tags = df['tags'].apply(to_drop))

df

df = df[df['drop_tags']==False]
df = df.drop(['drop_tags'], axis=1)
df.head()

#make every tag list element be a string 
def all_strings(lst):
  new = []
  for x in lst:
    if type(x)!=str:
      new.append(str(x))
    else:
      new.append(x)
  return new

df['tags'] = df['tags'].apply(all_strings)

x=list(df["speech"]) # To store the filtered clean_body values

def take_out_nan(lst):
  new = [] 
  for x in lst:
    if x != 'nan':
      new.append(x)
  return new

df['tags'] = df['tags'].apply(take_out_nan)

y=list(df["tags"]) # to store the corresponding tags

# Encode the tags(labels) in a binary format in order to be used for training
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
 
yt = mlb.fit_transform(y)
yt.shape

# Getting a sense of how the tags data looks like
print(mlb.classes_)

#Split data into Training, Validation and Test dataset
from sklearn.model_selection import train_test_split
# First Split for Train and Test
x_train,x_test,y_train,y_test = train_test_split(x, yt, test_size=0.1, random_state=RANDOM_SEED,shuffle=True)
# Next split Train in to training and validation
x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=RANDOM_SEED,shuffle=True)

y_test

"""# Preparing the Dataset and DataModule
First create QTagDataset class based on the Dataset class,that readies the text in a format needed for the BERT Model
"""

class QTagDataset (Dataset):
    def __init__(self,quest,tags, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.text = quest
        self.labels = tags
        self.max_len = max_len
        
    def __len__(self):
        return len(self.text)
    
    def __getitem__(self, item_idx):
        text = self.text[item_idx]
        inputs = self.tokenizer.encode_plus(
            text,
            None,
            add_special_tokens=True, # Add [CLS] [SEP]
            max_length= self.max_len,
            padding = 'max_length',
            return_token_type_ids= False,
            return_attention_mask= True, # Differentiates padded vs normal token
            truncation=True, # Truncate data beyond max length
            return_tensors = 'pt' # PyTorch Tensor format
          )
        
        input_ids = inputs['input_ids'].flatten()
        attn_mask = inputs['attention_mask'].flatten()
        #token_type_ids = inputs["token_type_ids"]
        
        return {
            'input_ids': input_ids ,
            'attention_mask': attn_mask,
            'label': torch.tensor(self.labels[item_idx], dtype=torch.float)
            
        }

class QTagDataModule (pl.LightningDataModule):
    
    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,batch_size=16,max_token_len=200):
        super().__init__()
        self.tr_text = x_tr
        self.tr_label = y_tr
        self.val_text = x_val
        self.val_label = y_val
        self.test_text = x_test
        self.test_label = y_test
        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.max_token_len = max_token_len

    def setup(self, stage = None):
        self.train_dataset = QTagDataset(quest=self.tr_text, tags=self.tr_label, tokenizer=self.tokenizer,max_len = self.max_token_len)
        self.val_dataset  = QTagDataset(quest=self.val_text,tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)
        self.test_dataset  = QTagDataset(quest=self.test_text,tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)
        
        
    def train_dataloader(self):
        return DataLoader (self.train_dataset,batch_size = self.batch_size,shuffle = True , num_workers=2)

    def val_dataloader(self):
        return DataLoader (self.val_dataset,batch_size= 16)

    def test_dataloader(self):
        return DataLoader (self.test_dataset,batch_size= 16)

# Initialize the Bert tokenizer //this is changable
BERT_MODEL_NAME = "bert-base-cased" # we will use the BERT base model(the smaller one)
Bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)

#questions = speeches
questions = x 
# For every sentence...
for question in questions:
    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    input_ids = Bert_tokenizer.encode(question, add_special_tokens=True)

# Initialize the parameters that will be use for training
N_EPOCHS = 20
BATCH_SIZE = 32
MAX_LEN = 300
LR = 2e-05

# Instantiate and set up the data_module
QTdata_module = QTagDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,Bert_tokenizer,BATCH_SIZE,MAX_LEN)
QTdata_module.setup()

"""# Train the Model"""

class QTagClassifier(pl.LightningModule):
    # Set up the classifier
    def __init__(self, n_classes=16, steps_per_epoch=None, n_epochs=3, lr=2e-5 ):
        super().__init__()

        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)
        self.classifier = nn.Linear(self.bert.config.hidden_size,n_classes) # outputs = number of labels
        self.steps_per_epoch = steps_per_epoch
        self.n_epochs = n_epochs
        self.lr = lr
        self.criterion = nn.BCEWithLogitsLoss()
        
    def forward(self,input_ids, attn_mask):
        output = self.bert(input_ids = input_ids ,attention_mask = attn_mask)
        output = self.classifier(output.pooler_output)
                
        return output
    
    
    def training_step(self,batch,batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['label']
        
        outputs = self(input_ids,attention_mask)
        loss = self.criterion(outputs,labels)
        self.log('train_loss',loss , prog_bar=True,logger=True)
        
        return {"loss" :loss, "predictions":outputs, "labels": labels }


    def validation_step(self,batch,batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['label']
        
        outputs = self(input_ids,attention_mask)
        loss = self.criterion(outputs,labels)
        self.log('val_loss',loss , prog_bar=True,logger=True)
        
        return loss

    def test_step(self,batch,batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['label']
        
        outputs = self(input_ids,attention_mask)
        loss = self.criterion(outputs,labels)
        self.log('test_loss',loss , prog_bar=True,logger=True)
        
        return loss
    
    
    def configure_optimizers(self):
        optimizer = AdamW(self.parameters() , lr=self.lr)
        warmup_steps = self.steps_per_epoch//3
        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps

        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)

        return [optimizer], [scheduler]

# Instantiate the classifier model
steps_per_epoch = len(x_tr)//BATCH_SIZE
model = QTagClassifier(n_classes=16, steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)

#Initialize Pytorch Lightning callback for Model checkpointing

# saves a file like: input/QTag-epoch=02-val_loss=0.32.ckpt
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',# monitored quantity
    filename='QTag-{epoch:02d}-{val_loss:.2f}',
    save_top_k=3, #  save the top 3 models
    mode='min', # mode of the monitored quantity  for optimization
)

# Instantiate the Model Trainer
#trainer = pl.Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback],progress_bar_refresh_rate = 30) threw error
# https://stackoverflow.com/questions/73264813/typeerror-init-got-an-unexpected-keyword-argument-progress-bar-refresh-r
trainer = pl.Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback])

"""https://stackoverflow.com/questions/71922261/typeerror-setup-got-an-unexpected-keyword-argument-stage"""

# Train the Classifier Model
trainer.fit(model, QTdata_module)

trainer.save(model)

pl.Trainer.save_model(f'/content/drive/MyDrive/Colab Notebooks/cPASS/Multi-Label Classifier(s)/Models/BERT_with_Irr_Cat')

#save trainer 
trainer.save_model('/content/drive/MyDrive/Colab Notebooks/cPASS/Multi-Label Classifier(s)/Models/BERT_with_Irr_Cat')

# Evaluate the model performance on the test dataset
trainer.test(model,datamodule=QTdata_module)

# Commented out IPython magic to ensure Python compatibility.
# Visualize the logs using tensorboard.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

"""# Evaluate Model Performance on Test Set"""

best_model_path

# Retreive the checkpoint path for best model
model_path = checkpoint_callback.best_model_path
model_path

# setup test dataset for BERT
from torch.utils.data import TensorDataset

# Tokenize all questions in x_test
input_ids = []
attention_masks = []


for quest in x_test:
    encoded_quest =  Bert_tokenizer.encode_plus(
                    quest,
                    None,
                    add_special_tokens=True,
                    max_length= MAX_LEN,
                    padding = 'max_length',
                    return_token_type_ids= False,
                    return_attention_mask= True,
                    truncation=True,
                    return_tensors = 'pt'      
    )
    
    # Add the input_ids from encoded question to the list.    
    input_ids.append(encoded_quest['input_ids'])
    # Add its attention mask 
    attention_masks.append(encoded_quest['attention_mask'])
    
# Now convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(y_test)

# Set the batch size.  
TEST_BATCH_SIZE = 64  

# Create the DataLoader.
pred_data = TensorDataset(input_ids, attention_masks, labels)
pred_sampler = SequentialSampler(pred_data)
pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=TEST_BATCH_SIZE)

"""# Prediction on test set"""

flat_pred_outs = 0
flat_true_labels = 0

# Put model in evaluation mode
model = model.to(device) # moving model to cuda
model.eval()

# Tracking variables 
pred_outs, true_labels = [], []
#i=0
# Predict 
for batch in pred_dataloader:
    # Add batch to GPU
    batch = tuple(t.to(device) for t in batch)
  
    # Unpack the inputs from our dataloader
    b_input_ids, b_attn_mask, b_labels = batch
 
    with torch.no_grad():
        # Forward pass, calculate logit predictions
        pred_out = model(b_input_ids,b_attn_mask)
        pred_out = torch.sigmoid(pred_out)
        # Move predicted output and labels to CPU
        pred_out = pred_out.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        #i+=1
        # Store predictions and true labels
        #print(i)
        #print(outputs)
        #print(logits)
        #print(label_ids)
    pred_outs.append(pred_out)
    true_labels.append(label_ids)

b_labels.shape

b_labels

b_attn_mask.shape

b_input_ids.shape

len(pred_outs)

pred_outs[0][0]

# Combine the results across all batches. 
flat_pred_outs = np.concatenate(pred_outs, axis=0)

# Combine the correct labels for each batch into a single list.
flat_true_labels = np.concatenate(true_labels, axis=0)

flat_pred_outs

flat_pred_outs.shape , flat_true_labels.shape

"""# Predictions of Tags in Test set"""

#define candidate threshold values
threshold  = np.arange(0.001,1,0.001)
threshold

from sklearn import metrics

# convert probabilities into 0 or 1 based on a threshold value
def ind_classify(pred_prob_lst,thresh):
    output_lst = []
    for i in pred_prob_lst:
      if i >= thresh:
        output_lst.append(1)
      else:
        output_lst.append(0)
    return output_lst

rel_columns = ['Adovcates_against_granting_more_authority',
 'Adovcates_against_restricting_authority' ,
 'Adovcates_for_granting_more_authority' ,
 'Adovcates_for_restricting_authority' ,
 'Advocates_against_Money_Military_aid_or_sanctions' ,
 'Advocates_against_Use_of_American_Air_Assets' ,
 'Advocates_against_Use_of_American_Ground_Troops' ,
 'Advocates_against_Use_of_American_Military_Force',
 'Advocates_against_Use_of_American_Naval_Assets',
 'Advocates_for_Money_Military_aid_or_sanctions',
 'Advocates_for_Use_of_American_Air_Assets',
 'Advocates_for_Use_of_American_Ground_Troops',
 'Advocates_for_Use_of_American_Military_Force',
 'Advocates_for_Use_of_American_Naval_Assets', 
 'President_already_has_sufficient_authority',
 'President_does_not_have_sufficient_authority']

#  ['Adovcates_against_granting_more_authority'
#  'Adovcates_against_restricting_authority'
#  'Adovcates_for_granting_more_authority'
#  'Adovcates_for_restricting_authority'
#  'Advocates_against_Money_Military_aid_or_sanctions'
#  'Advocates_against_Use_of_American_Air_Assets'
#  'Advocates_against_Use_of_American_Ground_Troops'
#  'Advocates_against_Use_of_American_Military_Force'
#  'Advocates_against_Use_of_American_Naval_Assets'
#  'Advocates_for_Money_Military_aid_or_sanctions'
#  'Advocates_for_Use_of_American_Air_Assets'
#  'Advocates_for_Use_of_American_Ground_Troops'
#  'Advocates_for_Use_of_American_Military_Force'
#  'Advocates_for_Use_of_American_Naval_Assets' 'Irrelevant'
#  'President_already_has_sufficient_authority'
#  'President_does_not_have_sufficient_authority']

 #goal make a master threshold list 
master_threshold_list = []
for i in range(16):
  mini_scores = []
  element_y_true = [label[i] for label in flat_true_labels]
  element_flat_pred_outs = [label[i] for label in flat_pred_outs]
  for thresh in threshold:
  #classes for each threshold
    mini_y_pred = ind_classify(element_flat_pred_outs,thresh) 
    mini_scores.append(metrics.f1_score(element_y_true, mini_y_pred, zero_division=1))

  optimal_thresh = threshold[mini_scores.index(max(mini_scores))]
  master_threshold_list.append(optimal_thresh)

master_threshold_list

#make a new classify function that does ONLY irrelavent score 
# ['Adovcates_against_granting_more_authority' 0
#  'Adovcates_against_restricting_authority' 1 
#  'Adovcates_for_granting_more_authority' 2 
#  'Adovcates_for_restricting_authority' 3 
#  'Advocates_against_Money_Military_aid_or_sanctions' 4
#  'Advocates_against_Use_of_American_Air_Assets' 5
#  'Advocates_against_Use_of_American_Ground_Troops' 6
#  'Advocates_against_Use_of_American_Military_Force'7
#  'Advocates_against_Use_of_American_Naval_Assets'8
#  'Advocates_for_Money_Military_aid_or_sanctions'9
#  'Advocates_for_Use_of_American_Air_Assets'10
#  'Advocates_for_Use_of_American_Ground_Troops'11
#  'Advocates_for_Use_of_American_Military_Force'12
#  'Advocates_for_Use_of_American_Naval_Assets'13 'Irrelevant'14
#  'President_already_has_sufficient_authority'15
#  'President_does_not_have_sufficient_authority'16 'nan'17]

"""# Score Eval

going to have a different optimal threshold for each columns
"""

flat_pred_outs

len(master_threshold_list)

flat_pred_outs

#apply each master threshold to each one in the flat pred outs 
master_threshold_list
y_pred_labels = []
for speech_labels in flat_pred_outs: 
  #17 bc last col was accident
  new_sentence = []
  for factor in range(16):
    #see if the value is greater = 
    if speech_labels[factor] >= master_threshold_list[factor]:
      new_sentence.append(1)
    else:
      new_sentence.append(0)
  y_pred_labels.append(new_sentence)

y_pred_labels

y_true = flat_true_labels

print(metrics.classification_report(y_true,y_pred_labels))

"""really bad f1 for 1,8,13,14"""



y_pred = mlb.inverse_transform(np.array(y_pred_labels))
y_act = mlb.inverse_transform(flat_true_labels)

df = pd.DataFrame({'Body':x_test,'Actual Tags':y_act,'Predicted Tags':y_pred})

df.tail()

"""# do outputs on actual test data"""

test_data_fp = '/content/drive/MyDrive/Colab Notebooks/cPASS/Multi-Label Classifier(s)/Part2_Rel_Input/Data/Rel_Or_Not_Predictions.csv'
# test data -- do this in a bit 
test_df = pd.read_csv(test_data_fp, dtype={'speech_id':'str'})
# drop all rows that are tweets
test_df = test_df[~test_df.tweet.astype('bool')]
# convert to boolean column
test_df.Evaluate = test_df.Evaluate == 'Yes'
# keep only rows to still evaluate
test_df = test_df[~test_df.Evaluate]

test_df.shape

test_df.tail()
test_df = test_df[["speech"]]
test_df.head()

# setup test dataset for BERT
from torch.utils.data import TensorDataset

# Tokenize all questions in x_test
input_ids = []
attention_masks = []


for quest in test_df['speech']:
    encoded_quest =  Bert_tokenizer.encode_plus(
                    quest,
                    None,
                    add_special_tokens=True,
                    max_length= MAX_LEN,
                    padding = 'max_length',
                    return_token_type_ids= False,
                    return_attention_mask= True,
                    truncation=True,
                    return_tensors = 'pt'      
    )
    
    # Add the input_ids from encoded question to the list.    
    input_ids.append(encoded_quest['input_ids'])
    # Add its attention mask 
    attention_masks.append(encoded_quest['attention_mask'])
    
# Now convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
#labels = torch.tensor(y_test)

# Set the batch size.  
TEST_BATCH_SIZE = 64  

# Create the DataLoader.
#pred_data = TensorDataset(input_ids, attention_masks, labels)
pred_data = TensorDataset(input_ids, attention_masks)
pred_sampler = SequentialSampler(pred_data)
pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=TEST_BATCH_SIZE)

pred_data.shape

y_test

labels.shape

attention_masks.shape

input_ids.shape

# Put model in evaluation mode
model = model.to(device) # moving model to cuda
model.eval()

# Tracking variables 
#pred_outs_big_test, true_labels = [], []
pred_outs_big_test = []
#i=0
# Predict 
for batch in pred_dataloader:
    # Add batch to GPU
    batch = tuple(t.to(device) for t in batch)
  
    # Unpack the inputs from our dataloader
    #b_input_ids, b_attn_mask, b_labels = batch
    b_input_ids, b_attn_mask = batch
 
    with torch.no_grad():
        # Forward pass, calculate logit predictions
        pred_out = model(b_input_ids,b_attn_mask)
        pred_out = torch.sigmoid(pred_out)
        # Move predicted output and labels to CPU
        pred_out = pred_out.detach().cpu().numpy()
    pred_outs_big_test.append(pred_out)
    # true_labels.append(label_ids)

len(pred_outs_big_test)

flat_pred_outs_big_test = np.concatenate(pred_outs_big_test, axis=0)

flat_pred_outs_big_test.shape

len(flat_pred_outs_big_test)

flat_pred_outs_big_test

#apply each master threshold to each one in the flat pred outs 
master_threshold_list
y_pred_labels_big_test = []
for speech_labels in flat_pred_outs_big_test: 
  #17 bc last col was accident
  new_sentence = []
  for factor in range(16):
    #see if the value is greater = 
    if speech_labels[factor] >= master_threshold_list[factor]:
      new_sentence.append(1)
    else:
      new_sentence.append(0)
  y_pred_labels_big_test.append(new_sentence)

y_pred_labels_big_test

big_pred = pd.DataFrame({'Speech':test_df['speech'],'Predicted Tags':mlb.inverse_transform(np.array(y_pred_labels_big_test))})
big_pred.head()

big_pred.shape

big_pred.to_csv(f'/content/drive/MyDrive/Colab Notebooks/output/Predictions_On_Uneval_11_19.csv')

