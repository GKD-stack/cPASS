# -*- coding: utf-8 -*-
"""WHOLE Relevant_Or_Not_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Niz9pBuw-I8PoX_CtU4VvH0Wlmdam5XF

### Setup
"""

# install packages
! pip install -q transformers
! pip install -q datasets
! pip install -q "ray[tune]"
! pip install -q hyperopt

import os
import numpy as np
import pandas as pd
import transformers as ppb
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""### Config"""

# file paths
data_fp = '/content/drive/MyDrive/Colab Notebooks/data/Combined.csv'
train_fp = 'data/train.csv'
val_fp = 'data/val.csv'
test_fp = 'data/test.csv'
train_drive_fp = '/content/drive/MyDrive/Colab Notebooks/data/preprocessed/train.csv'
val_drive_fp = '/content/drive/MyDrive/Colab Notebooks/data/preprocessed/val.csv'
test_drive_fp = '/content/drive/MyDrive/Colab Notebooks/data/preprocessed/test.csv'

# Variables
BATCH_SIZE = 16
LEARNING_RATE = 5e-5
# MODEL_NAME = 'allenai/longformer-base-4096'
MODEL_NAME = 'distilroberta-base'
# MODEL_NAME = 'distilbert-base-uncased'

# BATCH_SIZE = 8
# MODEL_NAME = 'bert-base-uncased'
# MODEL_NAME = 'roberta-base/'
# MODEL_NAME = 'roberta-large-mnli'
# MODEL_NAME = 'facebook/bart-large-mnli'
# MODEL_NAME = 'facebook/bart-base'
# MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'

"""# Data Cleaning

**Drop:**
* crisname, speech_id, MasterID, date, Scorer
* Rows where Evaluate is not 'Yes'
* Rows that contain Tweets

**Reformat:**
* Make a use of force relevant df or not 
* Evaluate: 1 if yes, 0 otherwise
* Relevent to Use of Force: 1 if any of the following are true:
    * 'Advocates_for_Use_of_American_Military_Force'
    * 'Advocates_for_Use_of_American_Ground_Troops',
    * 'Advocates_for_Use_of_American_Air_Assets'
    * 'Advocates_for_Use_of_American_Naval_Assets'
    * 'Advocates_against_Use_of_American_Military_Force'
    * 'Advocates_against_Use_of_American_Ground_Troops',
    * 'Advocates_against_Use_of_American_Air_Assets'
    * 'Advocates_against_Use_of_American_Naval_Assets'
    * 'Advocates_against_Money_Military_aid_or_sanctions'
"""

import re
from sklearn.model_selection import train_test_split

# load data
orig_data = pd.read_csv(data_fp, dtype={'speech_id':'str'})

# drop all rows that are tweets
data = orig_data[~orig_data.tweet.astype('bool')]

# convert to boolean column
data.Evaluate = data.Evaluate == 'Yes'

# make data we didn't evaluate test data 
test_data = data[~data.Evaluate]

# drop irrelevant columns
data = data.drop(['speech_id', 'date', 'Scorer'], axis=1)

#ensure speech is a string
data.speech = data.speech.astype('string')

# keep only rows that we evaluated
data = data[data.Evaluate]

"""#### Condense columns:
**0:** Relevant to Use of Force  
**1:** Not Relevant Use of Force  
"""

def label(row):
    if row.Relevant_UoF:
        return 1
    return 0

data['Relevant_UoF'] = data[[
    'Advocates_for_Use_of_American_Military_Force', 
    'Advocates_for_Use_of_American_Ground_Troops', 
    'Advocates_for_Use_of_American_Air_Assets',
    'Advocates_for_Use_of_American_Naval_Assets',
    'Advocates_against_Use_of_American_Military_Force', 
    'Advocates_against_Use_of_American_Ground_Troops', 
    'Advocates_against_Use_of_American_Air_Assets',
    'Advocates_against_Use_of_American_Naval_Assets',
]].any(axis='columns')

data['Relevant_UoF'] = data['Relevant_UoF'].replace({True:1, False:0})

data

"""##### Parse ID
* speaker_party
  * **False:** Republican  
  * **True:** Democrat
* chamber  
  * **False:** House  
  * **True:** Senate
"""

queryID = r'((?:[A-Z]+))(\d{3})(\w)' # 3 groups: name (non caputuring group), 3 digits, and letter
def parseID(text):
    result = re.search(queryID, text)
    sp = re.search(queryID, text).group(2) == '100' if re.search(queryID, text) else np.nan
    c = re.search(queryID, text).group(3) =='S'if re.search(queryID, text) else np.nan
    return pd.Series([sp, c])
  
data[['speaker_party', 'chamber']] = data.MasterID.apply(parseID)

# select only relevant columns
data = data[['crisno', 'crisname', 'MasterID', 'speech', 'CrisisStartDate', 'days_since_crisis_initiation', 'Relevant_UoF', 'speaker_party', 'chamber']]
data['label'] = data.apply(label, axis=1)

#function to limit num of words in speech near keyword 
query = r'((?:\S+\s){0,25})(\*{4}\s.+?\s\*{4})((?:\s\S+){0,200})' #can change number of words to use: 200
def find_keyword(text):
    result = re.search(
        query, text.translate(str.maketrans('', '', '.,!:;\n'))
    )
    if result:
        return result.group(0).replace('****', '').replace('  ', ' ')
    return re.search(
        r'^\S+(\s\S+){0,225}', text.translate(str.maketrans('', '', '.,!:;\n'))
    ) # returns first 225 words

# drop empty speeches
data = data[~data.speech.isna()]

#apply function to current dataset
data.speech = (
    data.speech.apply(find_keyword)
)

# split into train and validation datasets
train_data, val_data = train_test_split(
    data, train_size=0.9
)

data

# parseID and select relevant columns
test_data[['speaker_party', 'chamber']] = test_data.MasterID.apply(parseID)
test_data.speaker_party = test_data.speaker_party.astype(bool)
test_data.chamber = test_data.chamber.astype(bool)

test_data = test_data[['speech_id', 'speech', 'speaker_party', 'chamber', 'crisname']]
test_data = test_data[~test_data.speech.isna()]

#make sure their type is bool
test_data.chamber.astype(bool)
test_data.speaker_party.astype(bool)

#prepare test data speeches
test_data.speech = (
    test_data.speech.apply(find_keyword)
)

# write train and test datasets to files
os.makedirs('data', exist_ok=True)
train_data.to_csv(train_fp, index=False)
val_data.to_csv(val_fp, index=False)
test_data.to_csv(test_fp, index=False)

# export to drive
train_data.to_csv(train_drive_fp, index=False)
val_data.to_csv(val_drive_fp, index=False)
test_data.to_csv(test_drive_fp, index=False)

"""# Model

## Pretrain model
"""

# select model
checkpoint = MODEL_NAME
config = ppb.AutoConfig.from_pretrained(checkpoint)
config.num_labels = 3

# Load pretrained tokenizer and model
tokenizer = ppb.AutoTokenizer.from_pretrained(checkpoint)
model = ppb.AutoModelForSequenceClassification.from_config(config)

"""## Dataset"""

from datasets import load_dataset
import torch

# train = load_dataset('csv', data_files=train_fp)
# val = load_dataset('csv', data_files=val_fp)
# test = load_dataset('csv', data_files=test_fp)

# if loading from drive
train = load_dataset('csv', data_files=train_drive_fp)
val = load_dataset('csv', data_files=val_drive_fp)
test = load_dataset('csv', data_files=test_drive_fp)

def tokenize_function(e):
    return tokenizer(e["speech"], padding="max_length", truncation=True)

# tokenize data
train = train.map(tokenize_function, batched=True)
train = train.remove_columns(['speech'])
train.set_format(type='torch')

val = val.map(tokenize_function, batched=True)
val = val.remove_columns(['speech'])
val.set_format(type='torch')

test = test.map(tokenize_function, batched=True)
test = test.remove_columns(['speech'])
test.set_format(type='torch')

data_collator = ppb.DataCollatorWithPadding(tokenizer=tokenizer)

train

train_dataloader = torch.utils.data.DataLoader(
    train['train'], shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator
    )

val_dataloader = torch.utils.data.DataLoader(
    val['train'], batch_size=BATCH_SIZE, collate_fn=data_collator
    )

"""## Training"""

from tqdm.notebook import tqdm
from ray import tune
from datasets import load_metric

"""#### Train Configuration"""

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
# optimizer = ppb.AdamW(model.parameters(), lr=5e-5)
lr_scheduler = ppb.get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
metric = load_metric("accuracy")

# clear cache
torch.cuda.empty_cache()

# attempting to use ray-tune
def model_init():
    return ppb.AutoModelForSequenceClassification.from_config(config)
    # ppb.AutoModelForSequenceClassification.from_pretrained(
    #     'distilbert-base-uncased', return_dict=True)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=-1)
    return metric.compute(predictions=predictions, references=labels)

tune_config = {
        "per_device_train_batch_size": BATCH_SIZE,
        "per_device_eval_batch_size": BATCH_SIZE,
        "num_train_epochs": tune.choice([2, 3, 4, 5, 6]),
        "learning_rate": tune.uniform(1e-5, 5e-5),
        "warmup_steps": tune.choice([0, 100, 200, 300, 400, 500, 600]),
        "weight_decay": tune.uniform(0., 0.3),
    }

training_args = ppb.TrainingArguments(
    "test", 
    evaluation_strategy="epoch", 
    eval_steps=2000, 
    disable_tqdm=True, 
    per_device_train_batch_size = BATCH_SIZE,
    save_strategy = 'epoch'
)
trainer = ppb.Trainer(
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train["train"],
    eval_dataset=val["train"],
    model_init=model_init,
    compute_metrics=compute_metrics,
)

"""## Don't Run If Loading A Previous Model"""

from ray.tune.suggest.hyperopt import HyperOptSearch
from ray.tune.schedulers import ASHAScheduler
import hyperopt.hp as hp
import gc

# ---- TEMP FIX ----
import psutil
import ray
ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total

"""https://pytorch.org/docs/stable/backends.html#module-torch.backends.mps

Returns whether PyTorch is built with MPS support. Note that this doesnâ€™t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.
"""

#! pip install -q optuna
#import optuna

#def create_hpspace(trial):
    # Use trial to create your hyper parameter space based 
    # based on any conditon or loops !!
    #return {
            #'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),
            #'epochs': trial.suggest_int('epochs', 2, 3),
            #'batch_size': 32,
            #'num_batches_per_epoch': 50,
            #}

#class Objective (object):  
    #def __call__(self, trial):
        # Use create_hspace function
        #self.optuna_hp_space = create_hspace(trial)

#best_trial = trainer.hyperparameter_search(
    #direction="maximize", 
    #backend="ray", 
    #n_trials=10 # number of trials
#)

os.environ["TUNE_DISABLE_AUTO_CALLBACK_LOGGERS"] = "1"

torch.cuda.empty_cache() 
gc.collect()
best_trial = trainer.hyperparameter_search(
    hp_space=lambda _: tune_config,
    direction="maximize",
    backend="ray",
    search_alg=HyperOptSearch(metric="objective", mode="max"),
    scheduler=ASHAScheduler(metric="objective", mode="max"),
    resources_per_trial={"cpu": 2, "gpu": 1},
    n_trials=20,
    keep_checkpoints_num=1,
    verbose=0, # hide most outputs
    resume='AUTO'
    # TUNE_MAX_PENDING_TRIALS_PG=1,
)
# best_trial = tune.run(
#     trainer,
#     config = tune_config,
#     search_alg=HyperOptSearch(metric="objective", mode="max"),
#     scheduler=ASHAScheduler(metric="objective", mode="max"),
# )



best_trial

"""#### Train Using Best Parameters"""

best_trial

for n, v in best_trial.hyperparameters.items():
    setattr(trainer.args, n, v)

trainer.train()

pred, pred_label_id, pred_metrics = trainer.predict(val['train'])

"""#### Manual Optimization"""

# def evaluate():
#     metric = load_metric("accuracy")

#     model.eval()
#     for batch in val_dataloader:
#         batch = {k: v.to(device) for k, v in batch.items()}
#         with torch.no_grad():
#             outputs = model(**batch)
#         logits = outputs.logits
#         # predictions += list(np.array(batch_predictions.cpu()))
#         predictions = torch.argmax(logits, dim=-1)
#         metric.add_batch(predictions=predictions, references=batch["labels"])

#     print('metric:',metric.compute())
#     # predictions = np.array(predictions).flatten()
#     # cf_matrix = confusion_matrix(predictions, val_data.label, normalize='all')
#     # print('accuracy:', np.mean(predictions==val_data.label))
#     # sns.heatmap(cf_matrix, annot=True, cmap='Blues')
#     # plt.show()

# progress_bar = tqdm(range(num_training_steps))

# model.train()
# for epoch in range(num_epochs):
#     # model.train()
#     print('epoch:', epoch)
#     for batch in train_dataloader:
#         batch = {k: v.to(device) for k, v in batch.items()}
#         outputs = model(**batch)
#         loss = outputs.loss
#         loss.backward()

#         optimizer.step()
#         lr_scheduler.step()
#         optimizer.zero_grad()
#         progress_bar.update(1)
#     # evaluate()

"""Distilbert:
metric: {'accuracy': 0.7476923076923077}  
epoch: 1  
metric: {'accuracy': 0.742051282051282}  
epoch: 2  
metric: {'accuracy': 0.7266666666666667}  
epoch: 3  
metric: {'accuracy': 0.7307692307692307}  
epoch: 4  
metric: {'accuracy': 0.7307692307692307}

## Training: LR
"""

# save/load models to drive
version = 7
# uncomment below to save
trainer.save_model(f'/content/drive/MyDrive/Colab Notebooks/models/{MODEL_NAME}_{version}')
trainer.save_state(f'/content/drive/MyDrive/Colab Notebooks/trainer_states/{MODEL_NAME}_{version}')

# load model
loaded_model = ppb.AutoModelForSequenceClassification.from_pretrained(f'/content/drive/MyDrive/Colab Notebooks/models/{MODEL_NAME}_7')

x = lambda _ : loaded_model
trainer = ppb.Trainer(
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train["train"],
    eval_dataset=val["train"],
    model_init=x,
    compute_metrics=compute_metrics,
)

# predict on training data
pred, pred_label_id, pred_metrics = trainer.predict(val['train'])

"""#### Evaluation"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

# add additional features
df = pd.DataFrame(pred)
df[3] = val['train']['speaker_party']
df[4] = val['train']['chamber']
#df[5] = val['train']['crisname']

print(MODEL_NAME)
lr_clf = LogisticRegression()
lr_clf.fit(pred, pred_label_id)
print(lr_clf.score(pred, pred_label_id))
print(MODEL_NAME, 'with additional features')
lr_clf.fit(df, pred_label_id)
print(lr_clf.score(df, pred_label_id))

print(MODEL_NAME)
knn_clf = KNeighborsClassifier()
knn_clf.fit(pred, pred_label_id)
print(knn_clf.score(pred, pred_label_id))
cf_matrix = confusion_matrix(knn_clf.predict(pred), pred_label_id, normalize='true')
sns.heatmap(cf_matrix, annot=True, cmap='Blues')
plt.title(f'{MODEL_NAME} confusion matrix')
plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/models/{MODEL_NAME}_{version}/confusion_matrix.png')

print(MODEL_NAME, 'with additional features')
knn_clf.fit(df, pred_label_id)
print(knn_clf.score(df, pred_label_id))
cf_matrix = confusion_matrix(knn_clf.predict(df), pred_label_id, normalize='true')
sns.heatmap(cf_matrix, annot=True, cmap='Blues')
plt.title(f'{MODEL_NAME} confusion matrix')
plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/models/{MODEL_NAME}_{version}/confusion_matrix_with_features.png')

"""### Output"""

# predict on test data
pred, pred_label_id, pred_metrics = trainer.predict(test['train'])

df = pd.DataFrame(pred)
df[3] = test['train']['speaker_party']
df[4] = test['train']['chamber']
# df[5] = test['train']['crisname']

labels = pd.read_csv(test_drive_fp, dtype={'speech_id':'str'})

# from sklearn.decomposition import PCA

corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True)

df

labels['predicted_label'] = knn_clf.predict(df)
labels['predicted_label'] = labels['predicted_label'].astype('category')

labels.chamber.value_counts()

corr_matrix = labels.corr()
sns.heatmap(corr_matrix, annot=True)

# save features and predictions
labels.to_csv(f'/content/drive/MyDrive/Colab Notebooks/output/{MODEL_NAME}_predictions_{version}.csv')

labels = labels.drop('speech', axis=1)

result = orig_data.merge(labels, on='speech_id',how='left')

# save full csv
result.to_csv(f'/content/drive/MyDrive/Colab Notebooks/output/{MODEL_NAME}_result_{version}.csv')

